import streamlit as st
import os
import json
import pandas as pd
import logging
import sys
from datetime import datetime
import time
from src.ui_elements.simple_nav import create_sidebar_navigation

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="AIè§†é¢‘å¤§å¸ˆ - è§†é¢‘åˆ†æ",
    page_icon="ğŸ¬",
    layout="wide"
)

# å¼ºåˆ¶æ³¨å…¥éšè—é¡¶æ æ ·å¼
st.markdown("""
<style>
/* éšè—streamlitè‡ªå¸¦å¯¼èˆªå’Œå…¶ä»–UIå…ƒç´  */
[data-testid="stSidebarNav"], 
header[data-testid="stHeader"],
div[data-testid="stToolbar"],
div[data-testid="stDecoration"],
div[data-testid="stStatusWidget"],
#MainMenu,
footer {
    display: none !important;
}
</style>
""", unsafe_allow_html=True)

# ç›´æ¥å¯¼å…¥å·¥å…·ç±»
try:
    from utils.analyzer import VideoAnalyzer
    from utils.processor import VideoProcessor
except ImportError as e:
    st.error(f"å¯¼å…¥å·¥å…·æ¨¡å—å¤±è´¥: {e}")
    # å¤‡ç”¨å¯¼å…¥æ–¹å¼
    import importlib.util
    
    def import_from_file(module_name, file_path):
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        return module
    
    try:
        analyzer_path = os.path.join(project_root, "utils", "analyzer.py")
        processor_path = os.path.join(project_root, "utils", "processor.py")
        
        analyzer_module = import_from_file("analyzer", analyzer_path)
        processor_module = import_from_file("processor", processor_path)
        
        VideoAnalyzer = analyzer_module.VideoAnalyzer
        VideoProcessor = processor_module.VideoProcessor
    except Exception as e:
        st.error(f"å¤‡ç”¨å¯¼å…¥æ–¹å¼ä¹Ÿå¤±è´¥: {e}")

# å¯¼å…¥çƒ­è¯æœåŠ¡
from src.core.hot_words_service import HotWordsService

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ–‡ä»¶è·¯å¾„
ANALYSIS_RESULTS_DIR = os.path.join('data', 'video_analysis', 'results')

# é»˜è®¤çƒ­è¯è¡¨ID
DEFAULT_VOCABULARY_ID = "vocab-aivideo-4d73bdb1b5ef496d94f5104a957c012b"

# åˆå§‹åŒ–çƒ­è¯æœåŠ¡
hot_words_service = HotWordsService()

from src.ui_elements.custom_theme import set_custom_theme

def load_dimensions():
    """åŠ è½½å½“å‰ç»´åº¦ç»“æ„"""
    if 'dimensions' in st.session_state:
        return st.session_state.dimensions
    else:
        return {'title': "", 'level1': [], 'level2': {}}

def process_video_analysis(file, analysis_type, dimensions=None, keywords=None, vocabulary_id=None):
    """å¤„ç†è§†é¢‘åˆ†æ"""
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs(ANALYSIS_RESULTS_DIR, exist_ok=True)
    
    # è®°å½•ä½¿ç”¨çš„çƒ­è¯è¡¨ID
    if vocabulary_id:
        logger.info(f"ä½¿ç”¨çƒ­è¯è¡¨IDè¿›è¡Œè§†é¢‘åˆ†æ: {vocabulary_id}")
    
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(file)
        
        # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # åˆ†æç»“æœ
        results = {
            'type': analysis_type,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'matches': []
        }
        
        if analysis_type == "ç»´åº¦åˆ†æ":
            results['dimensions'] = dimensions
            
            # å¤„ç†æ¯ä¸ªç»´åº¦
            total_steps = len(dimensions.get('level1', []))
            for i, dim1 in enumerate(dimensions.get('level1', [])):
                status_text.text(f"æ­£åœ¨åˆ†æç»´åº¦: {dim1}")
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                time.sleep(0.5)
                
                # å¯¹äºæ¯ä¸ªäºŒçº§ç»´åº¦
                for dim2 in dimensions.get('level2', {}).get(dim1, []):
                    # æ¨¡æ‹ŸåŒ¹é…
                    # å®é™…æƒ…å†µä¸‹ï¼Œè¿™é‡Œåº”è¯¥æœ‰åŸºäºNLPæˆ–å…¶ä»–ç®—æ³•çš„åŒ¹é…é€»è¾‘
                    # è¿™é‡Œæˆ‘ä»¬åªæ˜¯éšæœºé€‰æ‹©å‡ æ¡è®°å½•ä½œä¸ºç¤ºä¾‹
                    matches = df.sample(min(3, len(df))).to_dict('records')
                    
                    for match in matches:
                        results['matches'].append({
                            'dimension_level1': dim1,
                            'dimension_level2': dim2,
                            'timestamp': match.get('timestamp', '00:00:00'),
                            'text': match.get('text', ''),
                            'score': 0.75  # æ¨¡æ‹ŸåŒ¹é…åˆ†æ•°
                        })
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress((i + 1) / total_steps)
        
        elif analysis_type == "å…³é”®è¯åˆ†æ":
            results['keywords'] = keywords
            
            # å¤„ç†æ¯ä¸ªå…³é”®è¯
            total_steps = len(keywords)
            for i, keyword in enumerate(keywords):
                status_text.text(f"æ­£åœ¨åˆ†æå…³é”®è¯: {keyword}")
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                time.sleep(0.5)
                
                # æ¨¡æ‹ŸåŒ¹é…
                matches = df[df['text'].str.contains(keyword, case=False, na=False)].to_dict('records')
                
                for match in matches:
                    results['matches'].append({
                        'keyword': keyword,
                        'timestamp': match.get('timestamp', '00:00:00'),
                        'text': match.get('text', ''),
                        'score': 0.85  # æ¨¡æ‹ŸåŒ¹é…åˆ†æ•°
                    })
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress((i + 1) / total_steps)
        
        # å®Œæˆå¤„ç†
        progress_bar.progress(100)
        status_text.text("åˆ†æå®Œæˆï¼")
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(ANALYSIS_RESULTS_DIR, f"analysis_{datetime.now().strftime('%Y%m%d%H%M%S')}.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results, result_file
    
    except Exception as e:
        logger.error(f"å¤„ç†è§†é¢‘åˆ†ææ—¶å‡ºé”™: {str(e)}")
        st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        return None, None

def show_analysis_results(results, result_file):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    if not results:
        return
    
    st.markdown("## åˆ†æç»“æœ")
    
    # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
    st.markdown(f"**åˆ†æç±»å‹**: {results['type']}")
    st.markdown(f"**åˆ†ææ—¶é—´**: {results['timestamp']}")
    st.markdown(f"**åŒ¹é…æ•°é‡**: {len(results['matches'])}")
    
    # ä¸‹è½½æŒ‰é’®
    with open(result_file, 'r', encoding='utf-8') as f:
        json_data = f.read()
        st.download_button(
            label="ä¸‹è½½åˆ†æç»“æœ (JSON)",
            data=json_data,
            file_name=os.path.basename(result_file),
            mime="application/json"
        )
    
    # æ˜¾ç¤ºåŒ¹é…ç»“æœ
    st.markdown("### åŒ¹é…è¯¦æƒ…")
    
    if results['type'] == "ç»´åº¦åˆ†æ":
        # æŒ‰ç»´åº¦åˆ†ç»„æ˜¾ç¤º
        for dim1 in results.get('dimensions', {}).get('level1', []):
            # è¿‡æ»¤å‡ºå½“å‰ä¸€çº§ç»´åº¦çš„åŒ¹é…
            dim1_matches = [m for m in results['matches'] if m['dimension_level1'] == dim1]
            
            if dim1_matches:
                with st.expander(f"{dim1} ({len(dim1_matches)}ä¸ªåŒ¹é…)", expanded=False):
                    # æŒ‰äºŒçº§ç»´åº¦åˆ†ç»„
                    for dim2 in results.get('dimensions', {}).get('level2', {}).get(dim1, []):
                        # è¿‡æ»¤å‡ºå½“å‰äºŒçº§ç»´åº¦çš„åŒ¹é…
                        dim2_matches = [m for m in dim1_matches if m['dimension_level2'] == dim2]
                        
                        if dim2_matches:
                            st.markdown(f"#### {dim2} ({len(dim2_matches)}ä¸ªåŒ¹é…)")
                            
                            # æ˜¾ç¤ºæ¯ä¸ªåŒ¹é…
                            for match in dim2_matches:
                                st.markdown(f"""
                                **æ—¶é—´ç‚¹**: {match['timestamp']}  
                                **åŒ¹é…åˆ†æ•°**: {match['score']:.2f}  
                                **æ–‡æœ¬**: {match['text']}  
                                ---
                                """)
    
    elif results['type'] == "å…³é”®è¯åˆ†æ":
        # æŒ‰å…³é”®è¯åˆ†ç»„æ˜¾ç¤º
        for keyword in results.get('keywords', []):
            # è¿‡æ»¤å‡ºå½“å‰å…³é”®è¯çš„åŒ¹é…
            keyword_matches = [m for m in results['matches'] if m['keyword'] == keyword]
            
            if keyword_matches:
                with st.expander(f"å…³é”®è¯: {keyword} ({len(keyword_matches)}ä¸ªåŒ¹é…)", expanded=False):
                    # æ˜¾ç¤ºæ¯ä¸ªåŒ¹é…
                    for match in keyword_matches:
                        st.markdown(f"""
                        **æ—¶é—´ç‚¹**: {match['timestamp']}  
                        **åŒ¹é…åˆ†æ•°**: {match['score']:.2f}  
                        **æ–‡æœ¬**: {match['text']}  
                        ---
                        """)

def show():
    """æ˜¾ç¤ºè§†é¢‘åˆ†æé¡µé¢"""
    # è®¾ç½®è‡ªå®šä¹‰ä¸»é¢˜
    set_custom_theme()
    
    # ä½¿ç”¨é€šç”¨å¯¼èˆªç»„ä»¶
    create_sidebar_navigation("è§†é¢‘åˆ†æ")
    
    # é¡µé¢ä¸»ä½“å†…å®¹
    st.title("è§†é¢‘åˆ†æ")
    
    # åŠ è½½é»˜è®¤çƒ­è¯è¡¨ID
    if "selected_vocabulary_id" not in st.session_state:
        # å°è¯•ä»é…ç½®ä¸­è·å–é»˜è®¤çƒ­è¯è¡¨ID
        try:
            default_id = hot_words_service.get_default_vocabulary_id()
            st.session_state.selected_vocabulary_id = default_id if default_id else DEFAULT_VOCABULARY_ID
            logger.info(f"å·²è®¾ç½®é»˜è®¤çƒ­è¯è¡¨ID: {st.session_state.selected_vocabulary_id}")
        except Exception as e:
            logger.error(f"åŠ è½½é»˜è®¤çƒ­è¯è¡¨å‡ºé”™: {str(e)}")
            st.session_state.selected_vocabulary_id = DEFAULT_VOCABULARY_ID
    
    # åˆ›å»ºé€‰é¡¹å¡
    upload_tab, analysis_tab = st.tabs(["ä¸Šä¼ è§†é¢‘", "åˆ†æè®¾ç½®"])
    
    # ä¸Šä¼ è§†é¢‘é€‰é¡¹å¡
    with upload_tab:
        st.header("ä¸Šä¼ è§†é¢‘")
        
        # æ·»åŠ çƒ­è¯è¡¨é€‰æ‹©
        st.subheader("è½¬å½•è®¾ç½®")
        
        # åŠ è½½æ‰€æœ‰å¯ç”¨çš„çƒ­è¯è¡¨
        with st.spinner("åŠ è½½çƒ­è¯è¡¨..."):
            vocabularies, error_msg = hot_words_service.check_cloud_hotwords()
        
        if vocabularies:
            # åˆ›å»ºçƒ­è¯è¡¨é€‰æ‹©ä¸‹æ‹‰æ¡†
            vocab_options = []
            vocab_display_names = []
            
            for vocab in vocabularies:
                vocab_id = vocab.get('vocabulary_id', '')
                vocab_name = vocab.get('name', vocab_id)
                vocab_options.append(vocab_id)
                vocab_display_names.append(f"{vocab_name} ({vocab_id})")
            
            # æ‰¾åˆ°å½“å‰é€‰ä¸­IDåœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•
            selected_index = 0
            if st.session_state.selected_vocabulary_id in vocab_options:
                selected_index = vocab_options.index(st.session_state.selected_vocabulary_id)
            
            # æ˜¾ç¤ºçƒ­è¯è¡¨é€‰æ‹©ä¸‹æ‹‰æ¡†
            selected_display = st.selectbox(
                "é€‰æ‹©ç”¨äºè§†é¢‘è½¬å½•çš„çƒ­è¯è¡¨",
                options=vocab_display_names,
                index=selected_index,
                help="çƒ­è¯è¡¨å°†ç”¨äºæé«˜ç‰¹å®šè¯æ±‡çš„è¯†åˆ«å‡†ç¡®ç‡"
            )
            
            # æ›´æ–°é€‰ä¸­çš„çƒ­è¯è¡¨ID
            selected_index = vocab_display_names.index(selected_display)
            st.session_state.selected_vocabulary_id = vocab_options[selected_index]
            
            # æ˜¾ç¤ºå½“å‰é€‰ä¸­çš„çƒ­è¯è¡¨IDä»¥åŠçƒ­è¯ç®¡ç†æç¤º
            current_hotword_name = selected_display.split(' (')[0]
            current_hotword_id = st.session_state.selected_vocabulary_id
            st.info(f"å½“å‰é€‰ä¸­çš„çƒ­è¯è¡¨: {current_hotword_id}ï¼Œè¦æ·»åŠ æˆ–ä¿®æ”¹çƒ­è¯è¡¨ï¼Œè¯·å‰å¾€ã€çƒ­è¯ç®¡ç†é¡µé¢ã€‘")
        else:
            if error_msg:
                st.warning(f"åŠ è½½çƒ­è¯è¡¨å¤±è´¥: {error_msg}")
            else:
                st.warning("æœªæ‰¾åˆ°å¯ç”¨çš„çƒ­è¯è¡¨ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®è¿›è¡Œè½¬å½•")
            
            # ä½¿ç”¨é»˜è®¤çƒ­è¯è¡¨ID
            st.session_state.selected_vocabulary_id = DEFAULT_VOCABULARY_ID
        
        uploaded_file = st.file_uploader("é€‰æ‹©è¦åˆ†æçš„è§†é¢‘æ–‡ä»¶", type=["mp4", "mov", "avi", "mkv"], help="æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼")
        
        if uploaded_file:
            # æ˜¾ç¤ºä¸Šä¼ çš„è§†é¢‘ä¿¡æ¯
            st.video(uploaded_file)
            st.info(f"æ–‡ä»¶å: {uploaded_file.name}, å¤§å°: {uploaded_file.size} å­—èŠ‚")
            
            # å°†ä¸Šä¼ çš„è§†é¢‘ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            temp_video_path = os.path.join("data", "temp", uploaded_file.name)
            with open(temp_video_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            st.success(f"è§†é¢‘å·²ä¿å­˜åˆ°: {temp_video_path}")
            
            # ä¿å­˜è§†é¢‘è·¯å¾„åˆ°ä¼šè¯çŠ¶æ€
            st.session_state.video_path = temp_video_path
    
    # åˆ†æè®¾ç½®é€‰é¡¹å¡
    with analysis_tab:
        st.header("åˆ†æè®¾ç½®")
        
        if 'video_path' not in st.session_state:
            st.warning("è¯·å…ˆä¸Šä¼ è§†é¢‘")
        else:
            # åˆ†æç±»å‹é€‰æ‹©
            analysis_type = st.radio("é€‰æ‹©åˆ†æç±»å‹", ["ç»´åº¦åˆ†æ", "å…³é”®è¯åˆ†æ"])
            
            if analysis_type == "ç»´åº¦åˆ†æ":
                # æ˜¾ç¤ºç»´åº¦é€‰æ‹©
                st.subheader("ç»´åº¦é€‰æ‹©")
                
                # åŠ è½½å½“å‰ç»´åº¦ç»“æ„
                dimensions = load_dimensions()
                
                if not dimensions or not dimensions.get('level1'):
                    st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†æç»´åº¦ã€‚è¯·å‰å¾€ç»´åº¦ç®¡ç†é¡µé¢åˆ›å»ºæˆ–åŠ è½½ç»´åº¦æ¨¡æ¿ã€‚")
                else:
                    st.markdown(f"**å½“å‰åŠ è½½çš„ç»´åº¦æ¨¡æ¿**: {dimensions.get('title', 'æœªå‘½å')}")
                    
                    # æ˜¾ç¤ºç»´åº¦åˆ—è¡¨
                    for dim1 in dimensions.get('level1', []):
                        with st.expander(f"{dim1}", expanded=False):
                            # æ˜¾ç¤ºäºŒçº§ç»´åº¦
                            dim2_list = dimensions.get('level2', {}).get(dim1, [])
                            if dim2_list:
                                st.markdown(", ".join(dim2_list))
                            else:
                                st.markdown("*æ— äºŒçº§ç»´åº¦*")
                    
                    # ç‚¹å‡»åˆ†ææŒ‰é’®
                    if st.button("å¼€å§‹ç»´åº¦åˆ†æ", key="dim_analysis_btn"):
                        # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†å¹¶ç”Ÿæˆç»“æœ
                        with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘åˆ†æ..."):
                            # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„è§†é¢‘å¤„ç†é€»è¾‘
                            # ç°åœ¨æˆ‘ä»¬åªæ˜¯æ¨¡æ‹Ÿä¸€ä¸ªCSVæ–‡ä»¶ä½œä¸ºè¾“å…¥
                            sample_data_path = os.path.join("data", "temp", "sample_subtitles.csv")
                            
                            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ ·æœ¬æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
                            if not os.path.exists(sample_data_path):
                                # åˆ›å»ºç›®å½•
                                os.makedirs(os.path.dirname(sample_data_path), exist_ok=True)
                                
                                # åˆ›å»ºæ ·æœ¬æ•°æ®
                                sample_data = pd.DataFrame({
                                    'timestamp': ['00:00:10', '00:00:20', '00:00:30', '00:00:40', '00:00:50'],
                                    'text': [
                                        'å“ç‰Œçš„å½±å“åŠ›æ­£åœ¨ä¸æ–­å¢é•¿',
                                        'æˆ‘ä»¬éœ€è¦æé«˜ç”¨æˆ·çš„å“ç‰Œè®¤çŸ¥åº¦',
                                        'ç”¨æˆ·ä½“éªŒæ˜¯æˆ‘ä»¬äº§å“çš„æ ¸å¿ƒç«äº‰åŠ›',
                                        'åˆ›æ–°æ˜¯æ¨åŠ¨å“ç‰Œå‘å‰å‘å±•çš„å…³é”®',
                                        'æˆ‘ä»¬çš„äº§å“è´¨é‡å¾—åˆ°äº†ç”¨æˆ·çš„é«˜åº¦è®¤å¯'
                                    ]
                                })
                                sample_data.to_csv(sample_data_path, index=False)
                            
                            # å¤„ç†åˆ†æ - ä¼ å…¥å½“å‰é€‰ä¸­çš„çƒ­è¯è¡¨ID
                            results, result_file = process_video_analysis(
                                sample_data_path, 
                                "ç»´åº¦åˆ†æ", 
                                dimensions,
                                vocabulary_id=st.session_state.selected_vocabulary_id
                            )
                            
                            # æ˜¾ç¤ºç»“æœ
                            if results:
                                show_analysis_results(results, result_file)
            
            elif analysis_type == "å…³é”®è¯åˆ†æ":
                # æ˜¾ç¤ºå…³é”®è¯è¾“å…¥
                st.subheader("å…³é”®è¯è®¾ç½®")
                keywords_input = st.text_area("è¾“å…¥å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=150)
                
                if keywords_input.strip():
                    # å¤„ç†å…³é”®è¯
                    keywords = [kw.strip() for kw in keywords_input.split('\n') if kw.strip()]
                    st.markdown(f"å·²è¾“å…¥ {len(keywords)} ä¸ªå…³é”®è¯")
                    
                    # ç‚¹å‡»åˆ†ææŒ‰é’®
                    if st.button("å¼€å§‹å…³é”®è¯åˆ†æ", key="kw_analysis_btn"):
                        # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†å¹¶ç”Ÿæˆç»“æœ
                        with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘åˆ†æ..."):
                            # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„è§†é¢‘å¤„ç†é€»è¾‘
                            # ç°åœ¨æˆ‘ä»¬åªæ˜¯æ¨¡æ‹Ÿä¸€ä¸ªCSVæ–‡ä»¶ä½œä¸ºè¾“å…¥
                            sample_data_path = os.path.join("data", "temp", "sample_subtitles.csv")
                            
                            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ ·æœ¬æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
                            if not os.path.exists(sample_data_path):
                                # åˆ›å»ºç›®å½•
                                os.makedirs(os.path.dirname(sample_data_path), exist_ok=True)
                                
                                # åˆ›å»ºæ ·æœ¬æ•°æ®
                                sample_data = pd.DataFrame({
                                    'timestamp': ['00:00:10', '00:00:20', '00:00:30', '00:00:40', '00:00:50'],
                                    'text': [
                                        'å“ç‰Œçš„å½±å“åŠ›æ­£åœ¨ä¸æ–­å¢é•¿',
                                        'æˆ‘ä»¬éœ€è¦æé«˜ç”¨æˆ·çš„å“ç‰Œè®¤çŸ¥åº¦',
                                        'ç”¨æˆ·ä½“éªŒæ˜¯æˆ‘ä»¬äº§å“çš„æ ¸å¿ƒç«äº‰åŠ›',
                                        'åˆ›æ–°æ˜¯æ¨åŠ¨å“ç‰Œå‘å‰å‘å±•çš„å…³é”®',
                                        'æˆ‘ä»¬çš„äº§å“è´¨é‡å¾—åˆ°äº†ç”¨æˆ·çš„é«˜åº¦è®¤å¯'
                                    ]
                                })
                                sample_data.to_csv(sample_data_path, index=False)
                            
                            # å¤„ç†åˆ†æ - ä¼ å…¥å½“å‰é€‰ä¸­çš„çƒ­è¯è¡¨ID
                            results, result_file = process_video_analysis(
                                sample_data_path, 
                                "å…³é”®è¯åˆ†æ", 
                                keywords=keywords,
                                vocabulary_id=st.session_state.selected_vocabulary_id
                            )
                            
                            # æ˜¾ç¤ºç»“æœ
                            if results:
                                show_analysis_results(results, result_file)
                else:
                    st.warning("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªå…³é”®è¯")

if __name__ == "__main__":
    show() 